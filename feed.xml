<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
<<<<<<< HEAD
    <title></title>
    <description></description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 07 Feb 2023 17:02:14 +0100</pubDate>
    <lastBuildDate>Tue, 07 Feb 2023 17:02:14 +0100</lastBuildDate>
    <generator>Jekyll v4.2.0</generator>
    
      <item>
        <title>Latent Dirichlet Allocation</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;&lt;label for=&quot;tm_intro&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;tm_intro&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_lda/topic_modeling_intro.jpg&quot; /&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Latent Dirichlet Allocation (LDA) is one of the most popular topic modeling &lt;label for=&quot;topic modeling&quot; class=&quot;margin-toggle sidenote-number&quot;&gt;&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;topic modeling&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;sidenote&quot;&gt;Topic modeling is a technique in natural language processing that aims to automatically identify the underlying themes or topics in a collection of text documents. &lt;/span&gt; methods out there, even though it has been published back in 2003 by &lt;em&gt;David Blei&lt;/em&gt;, &lt;em&gt;Andrew Ng&lt;/em&gt;, and &lt;em&gt;Michael Jordan&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;LDA produces two output&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a probability distribution on words to represent &lt;strong&gt;topics&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;a probability distribution on topics to summarize &lt;strong&gt;documents&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So imagine you have the following set of sentences (a sentence is just a very short document):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;I like going running because it makes me feel good&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Let’s go swimming tomorrow morning!&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;I’m going to play at a concert tonight&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;The other day I saw a great pianist in the city streets&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;I love working out while listening to rock music&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;LDA will find the following summarization for such sentences:&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Sentence&lt;/th&gt;
      &lt;th&gt;Topic A&lt;/th&gt;
      &lt;th&gt;Topic B&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;I like going running because it makes me feel good&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Let’s go swimming tomorrow morning!&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;I’m going to play at a concert tonight&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;20%&lt;/td&gt;
      &lt;td&gt;80%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;The other day I saw a great pianist in the city streets&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;I love working out while listening to rock music&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;50%&lt;/td&gt;
      &lt;td&gt;50%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;An the following representation for the two topics:&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Topic&lt;/th&gt;
      &lt;th&gt;running&lt;/th&gt;
      &lt;th&gt;swimming&lt;/th&gt;
      &lt;th&gt;play&lt;/th&gt;
      &lt;th&gt;concert&lt;/th&gt;
      &lt;th&gt;pianist&lt;/th&gt;
      &lt;th&gt;work out&lt;/th&gt;
      &lt;th&gt;rock&lt;/th&gt;
      &lt;th&gt;music&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Topic A&lt;/td&gt;
      &lt;td&gt;30%&lt;/td&gt;
      &lt;td&gt;30%&lt;/td&gt;
      &lt;td&gt;10%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;30%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Topic B&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;12%&lt;/td&gt;
      &lt;td&gt;22%&lt;/td&gt;
      &lt;td&gt;22%&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;22%&lt;/td&gt;
      &lt;td&gt;22%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Ok, so how do we obtain such output?&lt;/p&gt;

&lt;p&gt;Before diving into the algorithm let’s revise a couple of prerequisities.&lt;/p&gt;

&lt;h2 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h2&gt;

&lt;h3 id=&quot;poisson-distribution&quot;&gt;Poisson Distribution&lt;/h3&gt;

&lt;p&gt;&lt;label for=&quot;poisson&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;poisson&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_lda/call-center.jpg&quot; /&gt;&lt;br /&gt;Call center calls are a typical example of Poisson distribution&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event.&lt;/p&gt;

&lt;p&gt;Example:
A call center receives an average of 180 calls per hour, 24 hours a day. The calls are independent: receiving one does not change the probability of when the next one will arrive. The number of calls received during any minute can be modeled as a Poisson probability distribution with mean 3: &lt;br /&gt;
with high probability the call center will receive 2 or 3 calls in a specific minute, but there is a small chance it will receive 0 or even 6.&lt;/p&gt;

\[\begin{align}
P(\text{k calls in a minute}) &amp;amp;= \frac{\lambda^k\cdot e^{-\lambda}}{k!} = \frac{3^k\cdot e^{-3}}{k!}
\\
\\
P(\text{0 call in a minute}) &amp;amp;= \frac{3^0\cdot e^{-3}}{0!}=0.05
\\
P(\text{1 call in a minute}) &amp;amp;= \frac{3^1\cdot e^{-3}}{1!}=0.15
\\
P(\text{2 call in a minute}) &amp;amp;= \frac{3^2\cdot e^{-3}}{2!}=0.23
\\
P(\text{3 call in a minute}) &amp;amp;= \frac{3^3\cdot e^{-3}}{3!}=0.23
\\
P(\text{4 call in a minute}) &amp;amp;= \frac{3^4\cdot e^{-3}}{4!}=0.17
\\
\\
\vdots
\end{align}\]

&lt;p&gt;This distribution makes this assumptions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The occurrence of one event does not affect the probability that a second event will occur \(\to\) events occur independently.&lt;/li&gt;
  &lt;li&gt;Two events cannot occur at exactly the same instant; instead, at each very small sub-interval, either exactly one event occurs, or no event occurs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If these conditions are true, then \(k\) is a Poisson random variable, and the distribution of \(k\) is a Poisson distribution.&lt;/p&gt;

&lt;h3 id=&quot;dirichlet-distribution&quot;&gt;Dirichlet Distribution&lt;/h3&gt;

&lt;p&gt;&lt;label for=&quot;dirichlet plots&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;dirichlet plots&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_lda/dirichlet_plots.png&quot; /&gt;&lt;br /&gt;Some possible configurations for a Dirichlet distribution with 3 parameters. On top are shown the heat maps of the distribution, on the bottom are shown data points sampled from each configuration. Each vertex corresponds to an “entity”. In LDA entities are topics. Image credits to &lt;a href=&quot;https://gist.github.com/tboggs/8778945&quot;&gt;tboggs&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The Dirichlet distribution is a family of continuous multivariate probability distributions parametrized by a vector \(\alpha\) of positive reals.&lt;br /&gt;
I will explain it in a simple way, that I think makes it easy to grasp the rationale behind it.&lt;/p&gt;

&lt;p&gt;Do you remember the Beta distribution? It is a continuous distribution that is usually dubbed as “the probability distribution of probabilities”, because it can only take values between \(0\) and \(1\).  &lt;br /&gt;
The Beta distribution has two parameters: \(\alpha\) and \(\beta\).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\alpha\) &lt;br /&gt;
an integer, the higher it is, the higher the probability of success is&lt;/li&gt;
  &lt;li&gt;\(\beta\) &lt;br /&gt;
an integer, the higher it is, the higher the probability of failure is&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The Dirichlet distribution is a generalization of the Beta distribution. It is a probability distribution describing probabilities of outcomes. Instead of describing probability of one of two outcomes of a Bernoulli trial, like the Beta distribution does, it describes probability of \(K\) outcomes. &lt;br /&gt;
The Beta distribution is the special case of the Dirichlet distribution with \(K=2\).&lt;br /&gt;
The parameters are \(\alpha_1, \alpha_2, \dots, \alpha_K\) all strictly positive, defined analogously to \(\alpha\) and \(\beta\) of the Beta distribution.&lt;/p&gt;

&lt;h1 id=&quot;latent-dirichlet-allocation-in-depth&quot;&gt;Latent Dirichlet Allocation in depth&lt;/h1&gt;

&lt;h2 id=&quot;assumptions&quot;&gt;Assumptions&lt;/h2&gt;

&lt;p&gt;&lt;label for=&quot;bow&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;bow&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_lda/bag_of_words.png&quot; /&gt;&lt;br /&gt;My attempt at generating a bag of words with DALLE&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;“bag-of-words” assumption: the order of the words in a document doesn’t matter&lt;/li&gt;
  &lt;li&gt;documents are exchangeable: the specific ordering of the documents in a corpus can also be neglected&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A classic representation theorem due to de Finetti (1990) establishes that any collection of exchangeable random variables has a representation as a mixture distribution (a mixture of two or more probability distributions).&lt;/p&gt;

&lt;h2 id=&quot;notation-and-terminology&quot;&gt;Notation and terminology&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;A &lt;strong&gt;&lt;em&gt;&lt;u&gt;word&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; is the basic unit of discrete data, defined to be an item from a vocabulary indexed by \(\{1, \dots ,V\}\). &lt;br /&gt;
We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. &lt;br /&gt;
Thus, using superscripts to denote components, the \(v\)-th word in the vocabulary is represented by a vector \(w\) such that \(w^v=1\) and \(w^u=0\) for \(u\neq v\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;&lt;em&gt;&lt;u&gt;document&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; is a sequence of \(N\) words denoted by \(\mathbf{w}= (w_1,w_2, \dots,w_n)\), where \(w_n\) is the \(n\)-th word in the sequence.&lt;/li&gt;
  &lt;li&gt;A &lt;strong&gt;&lt;em&gt;&lt;u&gt;corpus&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; is a collection of \(M\) documents denoted by \(C = \{\mathbf{w_1}, \mathbf{w_2}, \dots, \mathbf{w_M}\}\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h2&gt;

&lt;h3 id=&quot;assumption-on-documents-generation&quot;&gt;Assumption on documents generation&lt;/h3&gt;

&lt;p&gt;LDA assumes that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words.&lt;/p&gt;

&lt;p&gt;&lt;label for=&quot;lda_diagram&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;lda_diagram&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_lda/lda_diagram.png&quot; /&gt;&lt;br /&gt;Graphical model representation of LDA. The boxes are “plates” representing replicates. The outer plate represents documents, while the inner plate represents the repeated choice of topics and words within a document.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;LDA assumes the following generative process for each document \(\textbf{w}\) in a corpus \(D\):&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Choose \(N \sim Poisson(\xi)\)&lt;/li&gt;
  &lt;li&gt;Choose \(\theta \sim Dir(\alpha)\)&lt;/li&gt;
  &lt;li&gt;For each of the \(N\) words \(w_n\)
    &lt;ol&gt;
      &lt;li&gt;Choose a topic 
\({z_n \sim Multinomial(\theta)}\)&lt;/li&gt;
      &lt;li&gt;Choose a word \(w_n\) from \(p(w_n|z_n,\beta)\): &lt;br /&gt;
   a multinomial probability conditioned on the topics \(z_n\).&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;\(N \sim Poisson(\xi)\) means that we are sampling a number of words given the average number of words \(\xi\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\theta \sim Dir(\alpha)\) means that we are sampling topic probabilities given the vector of Dirichlet parameters \(\alpha\). &lt;br /&gt;
  if the number of topics is \(3\), \(\theta\) might look like \(\theta = (0.3, 0.5, 0.2)\).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\({z_n \sim Multinomial(\theta)}\)  means that we are sampling one out of the \(N\) topics, using \(\theta\) as our probability distribution over topics.  &lt;br /&gt;
If we consider \(\theta = (0.3, 0.5, 0.2)\) we’ll have&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;\(z_n=z^1\) with \(30\%\) probability&lt;/li&gt;
      &lt;li&gt;\(z_n\) = \(z^2\) with \(50\%\) probability&lt;/li&gt;
      &lt;li&gt;\(z_n = z^3\) with \(20\%\) probability&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;where \(z^1, z^2, z^3\) are simply three variables that represent the \(3\) topics.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;\(\beta\) is a matrix of dimensionality \(k \times V\) (number of topics times number of words in the vocabulary) &lt;br /&gt;
where \({\beta_{ij}=p(w^j=1|z^i=1)}\).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Assuming the generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.&lt;/p&gt;

&lt;h3 id=&quot;collapsed-gibbs-sampling&quot;&gt;Collapsed Gibbs Sampling&lt;/h3&gt;

&lt;p&gt;So now suppose you have a set of documents. You’ve chosen some fixed number of \(K\) topics to discover, and want to use LDA to learn the topic representation of each document and the words associated to each topic. &lt;br /&gt;
How do you do this? one way, known as collapsed Gibbs sampling, is the following:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;1.&lt;/span&gt; Go through each document and randomly assign each word in the document to one of the K topics
&lt;span class=&quot;p&quot;&gt;2.&lt;/span&gt; for each document
&lt;span class=&quot;p&quot;&gt;	1.&lt;/span&gt; for each word w in the document
&lt;span class=&quot;p&quot;&gt;		2.&lt;/span&gt; reassign a new topic to w using the probability P
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where 
\(P = P(z_i=j|z_{-i},w_i,d_i) \propto \frac{C^{WT}_{w_ij}+ \eta}{\sum^{W}_{w=1}C^{WT}_{wj}+W\eta}\times \frac{C^{DT}_{d_ij}+\alpha}{\sum_{t=1}^TC^{DT}_{d_it}+T\alpha}\)&lt;/p&gt;

&lt;p&gt;left member of the expression:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(P(z_i=j)\) - the probability that token \(i\) is assigned to topic \(j\)&lt;/li&gt;
  &lt;li&gt;\(z_{-i}\) - represents topic assignments of all other tokens&lt;/li&gt;
  &lt;li&gt;\(w_i\) - Word (index) of the \(i\)-th token&lt;/li&gt;
  &lt;li&gt;\(d_i\) - Document containing the \(i\)-th token.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;right member of the expression:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(C^{WT}\) - the word-topic matrix&lt;/li&gt;
  &lt;li&gt;\(\sum_{w=1}^{W}C_{wj}^{WT}\) - total number of tokens (words) in each topic&lt;/li&gt;
  &lt;li&gt;\(C^{DT}\) - the document-topic matrix&lt;/li&gt;
  &lt;li&gt;\(\sum_{t=1}^{T}C_{d_it}^DT\) - total number of tokens (words) in document \(i\)&lt;/li&gt;
  &lt;li&gt;\(\eta\) - parameter that sets the topic distribution for the words. The higher the more spread out the words will be across he specified number of topics (\(K\))&lt;/li&gt;
  &lt;li&gt;\(\alpha\) - parameter that sets the topic distribution for the documents. The higher the more spread out the documents will be across the specified number of topics (\(K\))&lt;/li&gt;
  &lt;li&gt;\(W\) - the total number of words in the set of documents&lt;/li&gt;
  &lt;li&gt;\(T\) - number of topics, equivalent of the \(K\) we defined earlier&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can iterate over the algorithm defined above until we find a stable representation of the matrices \(C^{WT}\) and \(C^{DT}\).&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;&lt;label for=&quot;the_end&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;the_end&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/the_end_jungle_book.webp&quot; /&gt;&lt;br /&gt;The end scene of the Jungle book by Zoltan Korda UK, 1942&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In summary, Latent Dirichlet Allocation is a powerful tool for uncovering the underlying topics in a collection of text documents. We have seen how LDA works by analyzing the probability distribution of words in a text corpus and grouping them into different topics.&lt;/p&gt;

&lt;p&gt;LDA has a wide range of applications in various fields such as natural language processing, information retrieval, and text mining. In natural language processing, LDA can be used to classify text into different categories or to understand the underlying themes in a set of documents. In information retrieval, LDA can be used to improve search results by understanding the topics of documents and in text mining can be used to extract insights from unstructured data.&lt;/p&gt;

&lt;p&gt;Even though LDA is a very popular algorithm in the topic modeling realm, I suggest you to try out other more recent models, such as &lt;a href=&quot;https://github.com/ddangelov/Top2Vec&quot;&gt;Top2Vec&lt;/a&gt; and &lt;a href=&quot;https://github.com/MaartenGr/BERTopic&quot;&gt;BERTopic&lt;/a&gt;: The first one relies on applying the clustering algorithm HDBSCAN on high dimensional token embeddings (relying though on UMAP for dimensionaliy reduction), while BERTopic leverages transformers and a class-based TF-IDF procedure.&lt;/p&gt;

&lt;p&gt;Se you soon!&lt;/p&gt;

&lt;p&gt;Sources:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf&quot;&gt;Original paper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/&quot;&gt;Echen’s blogpost&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ethen8181.github.io/machine-learning/clustering_old/topic_model/LDA.html&quot;&gt;Latent Dirichlet Allocation Using Gibbs Sampling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 15 Jan 2023 00:00:00 +0100</pubDate>
        <link>/2023/01/15/lda.html</link>
        <guid isPermaLink="true">/2023/01/15/lda.html</guid>
        
        
      </item>
    
      <item>
        <title>Introduction to Reinforcement Learning</title>
        <description>&lt;h1 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h1&gt;

&lt;p&gt;&lt;label for=&quot;backflip&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;backflip&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/2022-12-03-Introduction-to-RL/back_flip.gif&quot; /&gt;&lt;br /&gt;An example of reinforcement learning: the agent learns optimally to backflip. &lt;br /&gt;
Source: &lt;a href=&quot;&quot;&gt;Composing Value Functions in Reinforcement Learning&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In this blogpost we will introduce the most important concepts in Reinforcement learning and provide some real world examples, to show you how and when it is currently being used.&lt;/p&gt;

&lt;p&gt;This series is based on the lectures that DeepMind held for UCL in 2020, they are freely available at &lt;a href=&quot;https://www.youtube.com/watch?v=TCCjZe0y4Qc&amp;amp;t=2641s&quot;&gt;this&lt;/a&gt; YouTube link. &lt;br /&gt;
I suggest you to watch these videos because they are extremely clear in my opinion. I chose to write this series of blogposts to offer a readable format of the content of DeepMind’s lectures.&lt;/p&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Reinforcement Learning is the branch of Computer Science that studies how to learn to perform tasks by modeling them as the maximization of a reward obtained by interacting with an environment.&lt;/p&gt;

&lt;p&gt;This interaction can be summarized in the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;label for=&quot;rl_diagram&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;rl_diagram&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/2022-12-03-Introduction-to-RL/agent_environment_diagram.png&quot; /&gt;&lt;br /&gt;Reinforcement Learning entities diagram&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Let’s describe it, introducing right away some terminology:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Agent&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
The computer system whose goal is to maximize a reward&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Environment&lt;/em&gt;&lt;/strong&gt; &lt;br /&gt;
The system the agent interacts with (by performing &lt;em&gt;actions&lt;/em&gt;) and learns from (by observing changes in the environment’s state and collecting rewards)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Reward&lt;/em&gt;&lt;/strong&gt; \((R_t)\)   &lt;br /&gt;
a scalar feedback signal that indicates how well the agent is doing at timestep \(t\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Shortly, at each timestep \(t\) the agent&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;receives from the environment an observation \(O_t\)&lt;/li&gt;
  &lt;li&gt;receives a reward \(R_t\)&lt;/li&gt;
  &lt;li&gt;executes an action \(A_t\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was a very high level description of the core entities involved in the reinforcement learning problem. But what is it used in? Here are some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://heli.stanford.edu/&quot;&gt;Fly a helicopter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1909.09571.pdf&quot;&gt;Manage an investment portfolio&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control&quot;&gt;Control a power station&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mathworks.com/help/reinforcement-learning/ug/train-biped-robot-to-walk-using-reinforcement-learning-agents.html&quot;&gt;Make a robot walk&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=V1eYniJ0Rnk&quot;&gt;Play video or board games&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let’s delve into a lower level description, where I will introduce the mathematical terminology that will be used throughout the next episodes.&lt;/p&gt;

&lt;h1 id=&quot;the-environment&quot;&gt;The Environment&lt;/h1&gt;

&lt;p&gt;It is a system in which you can perform actions and, consequently, it will&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;change its state&lt;/li&gt;
  &lt;li&gt;give you a reward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This type of system is modeled as a Markov decision process, namely a mathematical framework that models decision making problems.&lt;br /&gt;
Look at the image below. It represents a graph with three nodes \((S_0, S_1, S_2)\). &lt;br /&gt;
From each state you can perform some actions that will lead you to another state with a certain probability. We associate a reward (the orange arrows) to some of these actions.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/mdp.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;On your left, an example of Markov Decision Process. Source: Wikipedia.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Rigorously, a Markov Decision Process is defined as a&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a set of states \(S\) called the &lt;em&gt;set space&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;a set of actions \(A\) called the &lt;em&gt;action space&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;a transition probability distribution &lt;br /&gt;
\(P(s&apos;|s_t,a_t): S\times A \to S\) that tells you what is the probability to go to state \(s&apos;\) if you perform action \(a_t\) in state \(s_t\).&lt;/li&gt;
  &lt;li&gt;A reward function \(R(s_t, a_t, s&apos;):S \times A \times S \to \mathbb{R}\) that returns a scalar feedback (an immediate reward) when you perform action \(a_t\) in state \(s_t\) and end up in state \(s&apos;\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover, the transition probability distribution \(P\) must satisfy the Markov property:&lt;/p&gt;

\[P(s&apos; |s_t, a_t) = P(s&apos;|\mathcal{H}_t, a_t)\]

\[\begin{equation}
R(s&apos;|s_t, a_t) = R(s&apos;|\mathcal{H}_t, a_t)
\end{equation}\]

&lt;p&gt;This means that&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the state \(s&apos;\) into which we transition after performing action \(a_t\) from state \(s_t\)&lt;/li&gt;
  &lt;li&gt;the obtained reward \(r\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;are independent from the history of the game (namely, the sequence of actions, observations, and rewards obtained so far).&lt;/p&gt;

&lt;p&gt;In other words, \(s_t\) contains all the information we need to choose what action to perform next.&lt;/p&gt;

&lt;p&gt;We have described extensively how we model an environment in Reinforcement Learning. It’s important though to note that the agent could have full knowledge about the environment state or not.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Fully observable environment&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The environment is fully observable if 
\(S_t = O_t = \text{environment state}\)
This means that the agent’s state is equivalent to the environment’s state.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Partially observable environment&lt;/strong&gt;  &lt;br /&gt;
An environment is partially observarble if the observations are not Markovian.&lt;br /&gt;
The environment state can still be Markovian, but the agent does not know it.&lt;/p&gt;

    &lt;p&gt;In this case the agent state is a function of the history.&lt;/p&gt;

    &lt;p&gt;For instance \(S_t=O_t\), more generally:&lt;/p&gt;

\[S_{t+1}=u(S_t, A_t, R_{t+1}, O_{t+1})\]

    &lt;p&gt;The agent state is often much smaller the environment state.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;an-example-of-partially-observable-environment&quot;&gt;An example of Partially Observable Environment&lt;/h1&gt;

&lt;p&gt;Imagine you are looking at a maze from above.&lt;/p&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/labyrinth_full.png&quot; /&gt;&lt;figcaption class=&quot;maincolumn-figure&quot;&gt;On the left, the entire labyrinth seen from above: the full environment state.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/labyrinth_1.png&quot; width=&quot;48%&quot; /&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/labyrinth_2.png&quot; width=&quot;52%&quot; /&gt;
&lt;figcaption&gt;On the left, two potential state observations. The two observations are indistinguishable, but they refer to two different locations in the labyrinth.   
&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;These two agent states are not Markovian because the only way we have to understand whether we are on the left side or the right side of the labyrinth is to analyze the agent’s history, checking what was the observation at previous timesteps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/labyrinth_both.png&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You could not construct a Markov agent state in this maze.&lt;/p&gt;

&lt;p&gt;To deal with partial observability, an agent can construct a suitable state representation \(\to\) this is when the concepts of &lt;em&gt;value function&lt;/em&gt; and &lt;em&gt;action value function&lt;/em&gt; come in handy.&lt;/p&gt;

&lt;h1 id=&quot;the-value-function&quot;&gt;The Value Function&lt;/h1&gt;

&lt;p&gt;The value function is a function \(v(s): S \to \mathbb{R}\) that returns for each state \(s \in S\) the expected cumulative reward the agent will get by finding himself in such state.&lt;/p&gt;

\[\begin{equation}
\begin{split}
v(s) &amp;amp;= \mathbb{E}[G_t | S_t = s] \\
&amp;amp;= \mathbb{E}[R_{t+1} + R_{t+2} + R_{t+3} + \dots |S_t = s]
\end{split}
\end{equation}\]

&lt;h1 id=&quot;the-policy&quot;&gt;The Policy&lt;/h1&gt;

&lt;p&gt;A policy is a function \(\pi: S \to A\) that, given a state \(s\), tells the agent what is the optimal action \(a\) to perform (the optimal action is the one that will lead him to the state with the highest expected cumulative reward). &lt;br /&gt;
A policy can be deterministic (the state-action mapping is univoque) or stochastic. In this latter case you will get in output a probability distribution over the actions set \(A\).&lt;/p&gt;

&lt;h1 id=&quot;the-q-value&quot;&gt;The Q value&lt;/h1&gt;

&lt;p&gt;A function that maps a state action pair \((s, a)\) to the expected cumulative reward.&lt;/p&gt;

\[\begin{equation}
\begin{split}
q(s,a) &amp;amp;= \mathbb{E}[G_t|S_t=s, A_t=a] \\
&amp;amp;=\mathbb{E}[R_{t+1}+ R_{t+2}+ R_{t+3}+\dots | S_t=s, A_t=a]
\end{split}
\end{equation}\]

&lt;p&gt;That’s all for this introductory blogpost! See you in the next one.&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Dec 2022 00:00:00 +0100</pubDate>
        <link>/reinforcement/learning/2022/12/03/Introduction-to-RL.html</link>
        <guid isPermaLink="true">/reinforcement/learning/2022/12/03/Introduction-to-RL.html</guid>
        
        
        <category>Reinforcement</category>
        
        <category>Learning</category>
        
      </item>
    
      <item>
        <title>Sphinx Tutorial</title>
        <description>&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;topic&lt;/th&gt;
      &lt;th&gt;OS&lt;/th&gt;
      &lt;th&gt;required modules&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Generate Sphinx documentation for a Python project with the &lt;em&gt;ReadTheDocs&lt;/em&gt; theme.&lt;/td&gt;
      &lt;td&gt;Windows&lt;/td&gt;
      &lt;td&gt;venv&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;abstract&quot;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;&lt;label for=&quot;id&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;id&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_sphinx/sphinxdoc.png&quot; /&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Suppose that you have written a Python library and you want to share it with your colleagues. One of the most important things to do is to write a clear documentation. This tutorial will help you in achieving just that.&lt;/p&gt;

&lt;p&gt;Our sample project’s structure will look like this&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;my_library
	|
	|---my_module.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my_module.py&lt;/code&gt; contains the following code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyFirstClass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; class constructor &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; This method prints a welcome message &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hello &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; This method prints a goodbye message &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Goodbye &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;in the end we will obtain an HTML page that documents all the methods we have written above.&lt;/p&gt;

&lt;p&gt;Let’s start!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;create-a-virtual-environment-and-install-sphinx&quot;&gt;Create a virtual environment and install Sphinx&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt; &lt;label for=&quot;sphinx-installation&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;sphinx-installation&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_sphinx/sphinx-installation.png&quot; /&gt;&lt;br /&gt;A bunch of people installing a sphinx. Image rights: Dalle.&lt;/span&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Open a shell (you’ll be using this for all the next steps of this tutorial)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Go to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my_project&lt;/code&gt; folder&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find the path of your python executable&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;import os, sys; print(os.path.dirname(sys.executable))&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;It will output a path (I’ll refer to it as &lt;em&gt;MY_PATH&lt;/em&gt;). &lt;br /&gt;
Copy it, you will need it in the next step.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Create a virtual environment&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;MY_PATH/python &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; venv &lt;span class=&quot;s2&quot;&gt;&quot;venv-mylibrary&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;activate your virtual environment by running&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;venv-mylibrary/Scripts/activate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Run the following command to install Sphinx and some auxiliary libraries.&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;sphinx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;5.3.0 sphinxcontrib.applehelp&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.0.3 sphinx-rtd-theme&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;1.1.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;Now you have added sphinx to the requirements of your virtual environment&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;sphinx-quickstart&quot;&gt;Sphinx Quickstart&lt;/h1&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;label for=&quot;id&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;id&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_sphinx/sphinx-quickstart.png&quot; /&gt;&lt;br /&gt;A sphinx quick-starting a race I guess. Image rights: Dalle.&lt;/span&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Create a folder where you’ll put your documentation files. I like to call it &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docs&lt;/code&gt;.&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;docs
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Navigate to the docs directory&lt;/li&gt;
  &lt;li&gt;Run the following command to create the necessary Sphinx files and directories:&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sphinx-quickstart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command will prompt you for some information about your project, let’s see them  in detail:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt; Separate source and build directories (y/n) [n]:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I suggest you to write &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt;, It looks cleaner.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; Project name:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I suggest you to use the name of the project’s root folder.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; Author name&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;s&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Insert the author names in alphabetical order, separated by a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;,&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; Project release &lt;span class=&quot;o&quot;&gt;[]&lt;/span&gt;:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you press enter, it will default to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;0.0.0&lt;/code&gt;, otherwise specify a different version.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; Project languge &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;en]: 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Press enter, it will default to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;en&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Finished: An initial directory structure has been created.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;my_library
    |
    | - docs 
    |    |
    |    | ---------- source
    |    |				|
    |    |				| - conf.py
    |    |				| - index.rst
    |    | - make.bat
    |    | - Makefile
    |
    | - my_module.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;edit-confpy&quot;&gt;Edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf.py&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conf.py&lt;/code&gt; file let’s you specify what extensions, themes and auxiliary information to create your Sphinx documentation. 
Modify it as shown below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# find the row where html_theme is set and modify it as follows (if it doesn&apos;t exist create it from scratch)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;html_theme&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;sphinx_rtd_theme&apos;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# find the row that contains extensions = [] and modify it as follows:
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extensions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;sphinx.ext.autodoc&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;sphinx.ext.napoleon&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Add the path of the root directory of the library
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;abspath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;../../my_library&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;create-my_modulerst&quot;&gt;Create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my_module.rst&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;At the same level of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.rst&lt;/code&gt; create a new file called  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my_module.rst&lt;/code&gt; (give it the name of the module you want to document in Sphinx).&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;my_library
    |
    | - docs 
    |    |
    |    | ---------- source
    |    |				|
    |    |				| - conf.py
    |    |				| - index.rst
    |	 |				| - my_module.rst
    |    | - make.bat
    |    | - Makefile
    |
    | - my_module.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And populate it as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-rst&quot;&gt;MyFirstClass
================

Description of MyClassName

.. autoclass:: my_module.MyFirstClass
   :members:
   :undoc-members:
   :show-inheritance:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will generate the documentation for each public method of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MyClassName&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Repeat this process for all the classes/modules you have.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;edit-indexrst&quot;&gt;Edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.rst&lt;/code&gt;&lt;/h1&gt;

&lt;p&gt;Edit the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docs/source/index.rst&lt;/code&gt; to add to the table of content the link to the files you have just created.&lt;br /&gt;
Mind the empty line before &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my_module&lt;/code&gt;, it is necessary!&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-rst&quot;&gt;Welcome to MY_LIBRARY&apos;s documentation!
=======================================

This is a high level description of MY_LIBRARY, modify it as you like.

.. toctree::
   :maxdepth: 2
   :caption: Contents:
   
   my_module


.. _my_module: my_module.rst



Indices and tables
==================

* :ref:`genindex`
* :ref:`modindex`
* :ref:`search`
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;generate-html&quot;&gt;Generate HTML&lt;/h1&gt;

&lt;p&gt;Once you have populated all of the necessary files, you can generate the HTML documentation by running the following command from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docs&lt;/code&gt; directory:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;make html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This will create the HTML documentation for your package in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docs/_build/html&lt;/code&gt; directory. &lt;br /&gt;
You can view the documentation by opening the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.html&lt;/code&gt; in a web browser.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;&lt;label for=&quot;the_end&quot; class=&quot;margin-toggle&quot;&gt;⊕&lt;/label&gt;&lt;input type=&quot;checkbox&quot; id=&quot;the_end&quot; class=&quot;margin-toggle&quot; /&gt;&lt;span class=&quot;marginnote&quot;&gt;&lt;img class=&quot;fullwidth&quot; src=&quot;/assets/imgs_sphinx/the_end.png&quot; /&gt;&lt;br /&gt;A smiling anthropomorphic Sphinx in an elegant red jacket during a curtain call of a Broadway show in the style of Salvador Dalì. Image rights: Dalle.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;That’s it for this tutorial! &lt;br /&gt;
You can now publish your repository to &lt;a href=&quot;https://readthedocs.org/&quot;&gt;readthedocs&lt;/a&gt; itself, or in any static website hosting platform (like GitHub Pages).&lt;/p&gt;

&lt;p&gt;Suggested reads:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://sphinx-rtd-tutorial.readthedocs.io/en/latest/index.html&quot;&gt;Read The Docs “How to”&lt;/a&gt; - ReadTheDocs itself has written a tutorial on how to generate documentation using Sphinx. Go to the &lt;em&gt;Populating Our Documentation&lt;/em&gt; section.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sphinx-rtd-tutorial.readthedocs.io/en/latest/read-the-docs.html&quot;&gt;Publishing the documentation to ReadTheDocs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 12 Nov 2022 00:00:00 +0100</pubDate>
        <link>/2022/11/12/sphinx.html</link>
        <guid isPermaLink="true">/2022/11/12/sphinx.html</guid>
        
        <category>devops</category>
        
        
      </item>
    
=======
    <title>William Bonvini</title>
    <description>A Jekyll theme for content-rich sites</description>
    <link>/</link>
    <atom:link href="/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Tue, 17 Jan 2023 18:10:49 -0500</pubDate>
    <lastBuildDate>Tue, 17 Jan 2023 18:10:49 -0500</lastBuildDate>
    <generator>Jekyll v4.3.1</generator>
    
      <item>
        <title>Introduction to Reinforcement Learning</title>
        <description>&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;In this blogpost we will introduce the most important concepts in Reinforcement learning and provide some real world examples, to show you how and when it is currently being used.&lt;/p&gt;

&lt;p&gt;This series is based on the lectures that DeepMind held for UCL in 2020, they are freely available at &lt;a href=&quot;www.google.it&quot;&gt;this&lt;/a&gt; YouTube link. &lt;br /&gt;
I suggest you to watch these videos because they are extremely clear in my opinion. I chose to write this series of blogposts to offer a readable format of the content of DeepMind’s lectures. This is also the way I like to learn new things: First i study them, then I try to reformulate the concepts in a well organized and concise fashion.&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;Reinforcement Learning is the branch of Computer Science that studies how to learn to perform tasks by modeling them as the maximization of a reward obtained by interacting with an environment.&lt;/p&gt;

&lt;p&gt;This interaction can be summarized in the following diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/agent_environment_diagram.png&quot; alt=&quot;prova&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s describe it, introducing right away some terminology:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Agent&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
The computer system whose goal is to maximize a reward&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Environment&lt;/em&gt;&lt;/strong&gt; &lt;br /&gt;
The system the agent interacts with (by performing &lt;em&gt;actions&lt;/em&gt;) and learns from (by observing changes in the environment’s state and collecting rewards)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Reward&lt;/em&gt;&lt;/strong&gt; \((R_t)\)   &lt;br /&gt;
a scalar feedback signal that indicates how well the agent is doing at timestep \(t\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Shortly, at each timestep $t$ the agent&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;receives from the environment an observation \(O_t\)&lt;/li&gt;
  &lt;li&gt;receives a reward \(R_t\)&lt;/li&gt;
  &lt;li&gt;executes an action \(A_t\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was a very high level description of the core entities involved in the reinforcement learning problem. But what is it used in? Here are some examples:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://heli.stanford.edu/&quot;&gt;Fly a helicopter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1909.09571.pdf&quot;&gt;Manage an investment portfolio&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control&quot;&gt;Control a power station&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mathworks.com/help/reinforcement-learning/ug/train-biped-robot-to-walk-using-reinforcement-learning-agents.html&quot;&gt;Make a robot walk&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=V1eYniJ0Rnk&quot;&gt;Play video or board games&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now let’s delve into a lower level description, where I will introduce the mathematical terminology that will be used throughout the next episodes.&lt;/p&gt;

&lt;h3 id=&quot;the-environment&quot;&gt;The Environment&lt;/h3&gt;

&lt;p&gt;It is a system in which you can perform actions and, consequently, it will&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;change its state&lt;/li&gt;
  &lt;li&gt;give you a reward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This type of system is modeled as a Markov decision process, namely a mathematical framework that models decision making problems.&lt;br /&gt;
Look at the image below. It represents a graph with three nodes \((S_0, S_1, S_2)\). From each state you can perform some actions that will lead you to another state with a certain probability. We associate a reward (the orange arrows) to some of these actions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/mdp.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Image source: wikipedia)&lt;/p&gt;

&lt;p&gt;Rigorously, a Markov Decision Process is defined as a&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a set of states \(S\) called the &lt;em&gt;set space&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;a set of actions \(A\) called the &lt;em&gt;action space&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;a transition probability distribution $$P(s’&lt;/td&gt;
          &lt;td&gt;s_t,a_t): S\times A \to S\(that tells you what is the probability to go to state\)s’\(if you perform action\)a_t\(in state\)s_t$$.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;A reward function \(R(s_t, a_t, s&apos;):S \times A \times S \to \mathbb{R}\) that returns a scalar feedback (an immediate reward) when you perform action \(a_t\) in state \(s_t\) and end up in state \(s&apos;\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover, the transition probability distribution \(P\) must satisfy the Markov property
\(P(s&apos; |s_t, a_t) = P(s&apos;|\mathcal{H}_t, a_t)\)&lt;/p&gt;

\[\begin{equation}
R(s&apos;|s_t, a_t) = R(s&apos;|\mathcal{H}_t, a_t)
\end{equation}\]

&lt;p&gt;This means that&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the state \(s&apos;\) into which we transition after performing action $a_t$ from state \(s_t\)&lt;/li&gt;
  &lt;li&gt;the obtained reward \(r\)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;are independent from the history of the game (namely, the sequence of actions, obervations, and rewards obtained so far).&lt;/p&gt;

&lt;p&gt;In other words, \(s_t\) contains all the information we need to choose what action to perform next.&lt;/p&gt;

&lt;p&gt;We have described extensively how we model an environment in Reinforcement Learning. It’s important though to note that the agent could have full knowledge about the environment state or not.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Fully observable environment&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;The environments is fully observable if 
\(S_t = O_t = \text{environment state}\)
This means that the agent’s state is equivalent to the environment’s state.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Partially observable environment&lt;/strong&gt;  &lt;br /&gt;
An environment is partially observarble if the observations are not Markovian.&lt;br /&gt;
The environment state can still be Markovian, but the agent does not know it.&lt;/p&gt;

    &lt;p&gt;In this case the agent state is a function of the history.&lt;/p&gt;

    &lt;p&gt;For instance \(S_t=O_t\), more generally:&lt;br /&gt;
\(S_{t+1}=u(S_t, A_t, R_{t+1}, O_{t+1})\)&lt;/p&gt;

    &lt;p&gt;The agent state is often much smaller the environment state.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;an-example-of-partially-observable-environment&quot;&gt;An example of Partially Observable Environment&lt;/h3&gt;

&lt;p&gt;the full environment state of a maze:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/labyrinth_full.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A potential observation:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/labyrinth_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Another observation in a different location&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/labyrinth_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The two observations above are indistinguishable, but they refer to two different locations in the labyrinth. &lt;br /&gt;
These two agent states are not Markovian because the only way we have to understand whether we are on the left side or the right side of the labyrinth is to analyze the agent’s history, checking what was the observation at previous timesteps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/2022-12-03-Introduction-to-RL/labyrinth_both.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You could not construct a Markov agent state in this maze.&lt;/p&gt;

&lt;p&gt;To deal with partial observability, an agent can construct a suitable state representation $\to$ this is when the concepts of &lt;em&gt;value function&lt;/em&gt; and &lt;em&gt;action value function&lt;/em&gt; come in handy.&lt;/p&gt;

&lt;h3 id=&quot;the-value-function&quot;&gt;The Value Function&lt;/h3&gt;

&lt;p&gt;The value function is a function \(v(s): S \to \mathbb{R}\) that returns for each state \(s \in S\) the expected cumulative reward the agent will get by finding himself in such state.
$$
\begin{equation}
\begin{split}
v(s) &amp;amp;= \mathbb{E}[G_t | S_t = s] &lt;br /&gt;
&amp;amp;= \mathbb{E}[R_{t+1} + R_{t+2} + R_{t+3} + \dots |S_t = s]
\end{split}&lt;/p&gt;

&lt;p&gt;\end{equation}
$$&lt;/p&gt;

&lt;h3 id=&quot;the-policy&quot;&gt;The Policy&lt;/h3&gt;

&lt;p&gt;A policy is a function \(\pi: S \to A\) that, given a state \(s\), tells the agent what is the optimal action \(a\) to perform (the optimal action is the one that will lead him to the state with the highest expected cumulative reward). &lt;br /&gt;
A policy can be deterministic (the state-action mapping is univoque) or stochastic. In this latter case you will get in output a probability distribution over the actions set \(A\).&lt;/p&gt;

&lt;h3 id=&quot;the-q-value&quot;&gt;The Q value&lt;/h3&gt;

&lt;p&gt;A function that maps a state action pair \((s, a)\) to the expected cumulative reward.
\(\begin{equation}
\begin{split}
q(s,a) &amp;amp;= \mathbb{E}[G_t|S_t=s, A_t=a] \\
&amp;amp;=\mathbb{E}[R_{t+1}+ R_{t+2}+ R_{t+3}+\dots | S_t=s, A_t=a]
\end{split}
\end{equation}\)&lt;/p&gt;

&lt;p&gt;That’s all for this introductory blogpost! See you in the next one.&lt;/p&gt;
</description>
        <pubDate>Sat, 03 Dec 2022 00:00:00 -0500</pubDate>
        <link>/articles/22/Introduction-to-RL</link>
        <guid isPermaLink="true">/articles/22/Introduction-to-RL</guid>
        
        
        <category>Reinforcement</category>
        
        <category>Learning</category>
        
      </item>
    
>>>>>>> c01a4423521f8c3ee2f11816cbb4cb8616cfe67c
  </channel>
</rss>
